{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8172a77-cf83-4a0b-9d4b-aafff9409783",
   "metadata": {},
   "source": [
    "<table align=\"left\" width=100%>\n",
    "    <tr>\n",
    "        <td width=\"10%\">\n",
    "            <img src=\"../images/RA_Logo.png\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <div align=\"center\">\n",
    "                <font color=\"#21618C\" size=8px>\n",
    "                  <b> 9. Semantic Analysis </b>\n",
    "                </font>\n",
    "            </div>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6c8ff5-571f-4171-8894-426151570cec",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/vidyadharbendre/learn_nlp_using_examples/blob/main/notebooks/09_Semantic_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/vidyadharbendre/learn_nlp_using_examples/blob/main/notebooks/09_Semantic_Analysis.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f852baf-702c-4df4-9c77-985452ece7fa",
   "metadata": {},
   "source": [
    "## What is Semantic Analysis?\n",
    "Semantic Analysis is the process of understanding the meaning and interpretation of words, phrases, and sentences in context. It involves analyzing the relationships and meanings of words to determine the intended message of the text.\n",
    "\n",
    "## Why Semantic Analysis?\n",
    "Semantic Analysis is essential for:\n",
    "\n",
    "Improving the accuracy of NLP tasks like sentiment analysis, machine translation, and information retrieval.\n",
    "Enhancing the understanding of context and meaning in text.\n",
    "Supporting more sophisticated text analysis and interpretation.\n",
    "\n",
    "## How to Achieve Semantic Analysis Programmatically?\n",
    "Using SpaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fd2535a-5ad8-44d8-ac7f-81f7ddfd23f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.4\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Print the version of SpaCy installed\n",
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16b47d82-5492-4ee1-b3c9-a5539312e146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.4\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Print the version of SpaCy installed\n",
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98c668f7-64a5-4ebb-a3f7-a71cc7444d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.4\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Print the version of SpaCy installed\n",
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce1d2f36-6b7d-4e67-972f-2466b2e76b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: Apple, Vector: [-1.2311026  -1.1917264   0.15840489  0.35988116  0.6805322 ]\n",
      "Token: is, Vector: [-1.0027504  -0.25142258  0.28945386  0.7576598  -0.58256423]\n",
      "Token: looking, Vector: [-0.41153106  1.094019    0.73340464  0.08318283 -1.0782878 ]\n",
      "Token: at, Vector: [ 1.3064765   1.9255978   0.5234667  -1.0940402  -0.33415377]\n",
      "Token: buying, Vector: [-0.38275096  0.8829504  -0.06646706  0.16533871 -0.43682557]\n",
      "Token: a, Vector: [0.93501806 0.00710958 0.04742083 1.0769678  0.44846863]\n",
      "Token: U.K., Vector: [-0.28209645 -1.3021066  -0.2161325   1.3588122   0.06865764]\n",
      "Token: startup, Vector: [-0.7089213  -0.34339467  0.4557867  -0.7568426  -0.3378846 ]\n",
      "Token: for, Vector: [-0.05590749  0.15916066  1.2973515  -0.43173665 -0.79201484]\n",
      "Token: $, Vector: [-0.6496361   1.2837512   0.17862085  0.393975    1.7535145 ]\n",
      "Token: 1, Vector: [-1.1002016   0.63555384  5.3034716   0.27870944  3.1097717 ]\n",
      "Token: billion, Vector: [-1.4627483  0.5298873 -1.3364623  1.4983535  3.1686215]\n",
      "Token: ., Vector: [-1.1132907  -0.66679347  0.06082547 -1.6785736   0.310877  ]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load SpaCy's English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example text\n",
    "text = \"Apple is looking at buying a U.K. startup for $1 billion.\"\n",
    "\n",
    "# Process the text with SpaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Semantic analysis using SpaCy\n",
    "semantic_analysis_spacy = [(token.text, token.vector) for token in doc]\n",
    "\n",
    "# Display token text and vector\n",
    "for token_text, token_vector in semantic_analysis_spacy:\n",
    "    print(f\"Token: {token_text}, Vector: {token_vector[:5]}\")  # Displaying first 5 dimensions of the vector for brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eb6ade-cf08-49ce-8eac-cb8c5a7db33a",
   "metadata": {},
   "source": [
    "Using NLTK:\n",
    "NLTK does not have built-in capabilities for vector-based semantic analysis directly. However, you can use word2vec or GloVe pre-trained word vectors from external libraries like gensim to perform semantic analysis. Here's an example using gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e42ca89-4a6b-4e66-ba5a-1b260bc7f86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda list scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62e7cf54-91e9-416e-a719-4f46054b8c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a29cc74-b77e-4732-a452-5a150090571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install --force-reinstall scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "942c302c-afed-46f7-94ea-d79177959bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "print(scipy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92ac5a3b-6ce9-437f-9d98-3e3d6c85f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda update -n nlp_env scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7729c541-9833-4708-ad06-424cf2f8adc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import triu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "339aa950-25ba-4e63-b16c-a27584ef6783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Matrix:\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "Upper Triangular Part:\n",
      "[[1 2 3]\n",
      " [0 5 6]\n",
      " [0 0 9]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import triu\n",
    "\n",
    "# Create a sample matrix\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "\n",
    "# Extract the upper triangular part of the matrix\n",
    "upper_triangular = triu(A)\n",
    "\n",
    "print(\"Original Matrix:\")\n",
    "print(A)\n",
    "print(\"\\nUpper Triangular Part:\")\n",
    "print(upper_triangular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fd524ba-e0d2-4761-8f92-bc5b748c629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import triu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63e19554-153e-499e-a51f-d5da8a9827dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'how', 'are', 'you', '?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/vidyadharbendre/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Ensure necessary resources are downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample text\n",
    "text = \"Hello, how are you?\"\n",
    "\n",
    "# Tokenize text\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9f4af42-03e9-4692-a925-c8d7c5deb4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/vidyadharbendre/nlp_workspace/learn_nlp_using_examples/GoogleNews-vectors-negative300.bin'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path_to_model = '/Users/vidyadharbendre/nlp_workspace/learn_nlp_using_examples/GoogleNews-vectors-negative300.bin'\n",
    "path_to_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1078fc9-3866-41b8-9664-ba3f894d8a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(path_to_model):\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format(path_to_model, binary=True)\n",
    "else:\n",
    "    print(f\"Error: File '{path_to_model}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1552b0ea-b8f0-4e3d-a455-dd3847b9042b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/vidyadharbendre/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: Apple, Vector: [-0.17480469  0.0300293  -0.21679688  0.15625    -0.35742188]\n",
      "Word: is, Vector: [ 0.00704956 -0.07324219  0.171875    0.02258301 -0.1328125 ]\n",
      "Word: looking, Vector: [ 0.02783203  0.25585938  0.15820312 -0.0480957  -0.05395508]\n",
      "Word: at, Vector: [-0.05859375 -0.03759766  0.07275391  0.10888672  0.06640625]\n",
      "Word: buying, Vector: [ 0.12109375  0.05664062 -0.2421875   0.18164062 -0.03637695]\n",
      "Word: U.K., Vector: [ 0.11328125  0.07373047 -0.2890625   0.05639648 -0.11474609]\n",
      "Word: startup, Vector: [ 0.0390625  -0.08251953  0.00311279 -0.07373047  0.10791016]\n",
      "Word: for, Vector: [-0.01177979 -0.04736328  0.04467773  0.06347656 -0.01818848]\n",
      "Word: $, Vector: [ 0.11376953 -0.11767578  0.06494141  0.1328125   0.05493164]\n",
      "Word: 1, Vector: [ 0.05078125 -0.09326172  0.06494141  0.11425781 -0.06494141]\n",
      "Word: billion, Vector: [0.04541016 0.02648926 0.09960938 0.13964844 0.01708984]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Ensure necessary resources are downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load pre-trained Word2Vec model (for example purposes, using a small pre-trained model)\n",
    "# You can download a larger model from gensim-data or use your own pre-trained model\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(path_to_model, binary=True)\n",
    "\n",
    "# Example text\n",
    "text = \"Apple is looking at buying a U.K. startup for $1 billion.\"\n",
    "\n",
    "# Tokenize the text\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Semantic analysis using Word2Vec\n",
    "semantic_analysis_word2vec = [(word, word2vec_model[word]) for word in words if word in word2vec_model]\n",
    "\n",
    "# Display token text and vector\n",
    "for word, vector in semantic_analysis_word2vec:\n",
    "    print(f\"Word: {word}, Vector: {vector[:5]}\")  # Displaying first 5 dimensions of the vector for brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1beff1-c943-4d7f-b60c-6416eaf836cd",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "\n",
    "What: This snippet demonstrates semantic analysis using NLTK for tokenization and Gensim's Word2Vec model for obtaining word vectors.\n",
    "\n",
    "Why: NLTK is used for tokenization (word_tokenize) to break down the text into words. Gensim's Word2Vec model (KeyedVectors) is then loaded to obtain vector representations for each word.\n",
    "\n",
    "How: After tokenizing the text (words = word_tokenize(text)), the script checks if each word exists in the loaded Word2Vec model (word in word2vec_model). If true, it retrieves the vector representation (word2vec_model[word]) for semantic analysis purposes.\n",
    "\n",
    "## Summary\n",
    "SpaCy offers an efficient way to perform semantic analysis by leveraging pre-trained word vectors.\n",
    "NLTK provides tools for text processing and tokenization, which are essential for preparing text for semantic analysis.\n",
    "Gensim's Word2Vec models enable the extraction of semantic information through word vectors, enhancing the understanding of textual context and meaning.\n",
    "By using these libraries and models, developers can implement robust semantic analysis solutions for various NLP tasks effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a394e204-6565-48e4-ad2e-ead4e459a5ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b19740c-9d67-4a1e-a6f0-794ad2b3d95f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
