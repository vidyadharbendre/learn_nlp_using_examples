{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8172a77-cf83-4a0b-9d4b-aafff9409783",
   "metadata": {},
   "source": [
    "<table align=\"left\" width=100%>\n",
    "    <tr>\n",
    "        <td width=\"10%\">\n",
    "            <img src=\"../images/RA_Logo.png\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <div align=\"center\">\n",
    "                <font color=\"#21618C\" size=8px>\n",
    "                  <b> 9. Semantic Analysis </b>\n",
    "                </font>\n",
    "            </div>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6c8ff5-571f-4171-8894-426151570cec",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/vidyadharbendre/learn_nlp_using_examples/blob/main/notebooks/09_Semantic_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/vidyadharbendre/learn_nlp_using_examples/blob/main/notebooks/09_Semantic_Analysis.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f852baf-702c-4df4-9c77-985452ece7fa",
   "metadata": {},
   "source": [
    "## What is Semantic Analysis?\n",
    "Semantic Analysis is the process of understanding the meaning and interpretation of words, phrases, and sentences in context. It involves analyzing the relationships and meanings of words to determine the intended message of the text.\n",
    "\n",
    "Sentiment Analysis is the process of determining the sentiment expressed in a piece of text, whether it is positive, negative, or neutral. It involves analyzing the subjective information conveyed in the text to understand the emotions and opinions of the author.\n",
    "\n",
    "## Why Semantic Analysis?\n",
    "Semantic Analysis is essential for:\n",
    "\n",
    "Improving the accuracy of NLP tasks like sentiment analysis, machine translation, and information retrieval.\n",
    "Enhancing the understanding of context and meaning in text.\n",
    "Supporting more sophisticated text analysis and interpretation.\n",
    "\n",
    "Sentiment Analysis is crucial for various applications:\n",
    "\n",
    "Understanding Customer Feedback: Analyzing customer reviews and social media posts to gauge customer sentiment towards products or services.\n",
    "Brand Monitoring: Monitoring sentiment towards a brand or product to manage reputation and customer satisfaction.\n",
    "Market Research: Analyzing sentiment in news articles or financial reports to gauge market trends and investor sentiment.\n",
    "\n",
    "## How to Achieve Semantic Analysis Programmatically?\n",
    "Using SpaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fd2535a-5ad8-44d8-ac7f-81f7ddfd23f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.4\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Print the version of SpaCy installed\n",
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16b47d82-5492-4ee1-b3c9-a5539312e146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.4\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Print the version of SpaCy installed\n",
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98c668f7-64a5-4ebb-a3f7-a71cc7444d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.4\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Print the version of SpaCy installed\n",
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce1d2f36-6b7d-4e67-972f-2466b2e76b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: Apple, Vector: [-1.2311026  -1.1917264   0.15840489  0.35988116  0.6805322 ]\n",
      "Token: is, Vector: [-1.0027504  -0.25142258  0.28945386  0.7576598  -0.58256423]\n",
      "Token: looking, Vector: [-0.41153106  1.094019    0.73340464  0.08318283 -1.0782878 ]\n",
      "Token: at, Vector: [ 1.3064765   1.9255978   0.5234667  -1.0940402  -0.33415377]\n",
      "Token: buying, Vector: [-0.38275096  0.8829504  -0.06646706  0.16533871 -0.43682557]\n",
      "Token: a, Vector: [0.93501806 0.00710958 0.04742083 1.0769678  0.44846863]\n",
      "Token: U.K., Vector: [-0.28209645 -1.3021066  -0.2161325   1.3588122   0.06865764]\n",
      "Token: startup, Vector: [-0.7089213  -0.34339467  0.4557867  -0.7568426  -0.3378846 ]\n",
      "Token: for, Vector: [-0.05590749  0.15916066  1.2973515  -0.43173665 -0.79201484]\n",
      "Token: $, Vector: [-0.6496361   1.2837512   0.17862085  0.393975    1.7535145 ]\n",
      "Token: 1, Vector: [-1.1002016   0.63555384  5.3034716   0.27870944  3.1097717 ]\n",
      "Token: billion, Vector: [-1.4627483  0.5298873 -1.3364623  1.4983535  3.1686215]\n",
      "Token: ., Vector: [-1.1132907  -0.66679347  0.06082547 -1.6785736   0.310877  ]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load SpaCy's English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example text\n",
    "text = \"Apple is looking at buying a U.K. startup for $1 billion.\"\n",
    "\n",
    "# Process the text with SpaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Semantic analysis using SpaCy\n",
    "semantic_analysis_spacy = [(token.text, token.vector) for token in doc]\n",
    "\n",
    "# Display token text and vector\n",
    "for token_text, token_vector in semantic_analysis_spacy:\n",
    "    print(f\"Token: {token_text}, Vector: {token_vector[:5]}\")  # Displaying first 5 dimensions of the vector for brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eb6ade-cf08-49ce-8eac-cb8c5a7db33a",
   "metadata": {},
   "source": [
    "Using NLTK:\n",
    "NLTK does not have built-in capabilities for vector-based semantic analysis directly. However, you can use word2vec or GloVe pre-trained word vectors from external libraries like gensim to perform semantic analysis. Here's an example using gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e42ca89-4a6b-4e66-ba5a-1b260bc7f86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda list scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62e7cf54-91e9-416e-a719-4f46054b8c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a29cc74-b77e-4732-a452-5a150090571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install --force-reinstall scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "942c302c-afed-46f7-94ea-d79177959bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "print(scipy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92ac5a3b-6ce9-437f-9d98-3e3d6c85f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda update -n nlp_env scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7729c541-9833-4708-ad06-424cf2f8adc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import triu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "339aa950-25ba-4e63-b16c-a27584ef6783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Matrix:\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "Upper Triangular Part:\n",
      "[[1 2 3]\n",
      " [0 5 6]\n",
      " [0 0 9]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import triu\n",
    "\n",
    "# Create a sample matrix\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "\n",
    "# Extract the upper triangular part of the matrix\n",
    "upper_triangular = triu(A)\n",
    "\n",
    "print(\"Original Matrix:\")\n",
    "print(A)\n",
    "print(\"\\nUpper Triangular Part:\")\n",
    "print(upper_triangular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fd524ba-e0d2-4761-8f92-bc5b748c629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import triu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63e19554-153e-499e-a51f-d5da8a9827dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'how', 'are', 'you', '?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/vidyadharbendre/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Ensure necessary resources are downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample text\n",
    "text = \"Hello, how are you?\"\n",
    "\n",
    "# Tokenize text\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9f4af42-03e9-4692-a925-c8d7c5deb4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/vidyadharbendre/nlp_workspace/learn_nlp_using_examples/GoogleNews-vectors-negative300.bin'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path_to_model = '/Users/vidyadharbendre/nlp_workspace/learn_nlp_using_examples/GoogleNews-vectors-negative300.bin'\n",
    "path_to_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1078fc9-3866-41b8-9664-ba3f894d8a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(path_to_model):\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format(path_to_model, binary=True)\n",
    "else:\n",
    "    print(f\"Error: File '{path_to_model}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1552b0ea-b8f0-4e3d-a455-dd3847b9042b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/vidyadharbendre/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: Apple, Vector: [-0.17480469  0.0300293  -0.21679688  0.15625    -0.35742188]\n",
      "Word: is, Vector: [ 0.00704956 -0.07324219  0.171875    0.02258301 -0.1328125 ]\n",
      "Word: looking, Vector: [ 0.02783203  0.25585938  0.15820312 -0.0480957  -0.05395508]\n",
      "Word: at, Vector: [-0.05859375 -0.03759766  0.07275391  0.10888672  0.06640625]\n",
      "Word: buying, Vector: [ 0.12109375  0.05664062 -0.2421875   0.18164062 -0.03637695]\n",
      "Word: U.K., Vector: [ 0.11328125  0.07373047 -0.2890625   0.05639648 -0.11474609]\n",
      "Word: startup, Vector: [ 0.0390625  -0.08251953  0.00311279 -0.07373047  0.10791016]\n",
      "Word: for, Vector: [-0.01177979 -0.04736328  0.04467773  0.06347656 -0.01818848]\n",
      "Word: $, Vector: [ 0.11376953 -0.11767578  0.06494141  0.1328125   0.05493164]\n",
      "Word: 1, Vector: [ 0.05078125 -0.09326172  0.06494141  0.11425781 -0.06494141]\n",
      "Word: billion, Vector: [0.04541016 0.02648926 0.09960938 0.13964844 0.01708984]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Ensure necessary resources are downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load pre-trained Word2Vec model (for example purposes, using a small pre-trained model)\n",
    "# You can download a larger model from gensim-data or use your own pre-trained model\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(path_to_model, binary=True)\n",
    "\n",
    "# Example text\n",
    "text = \"Apple is looking at buying a U.K. startup for $1 billion.\"\n",
    "\n",
    "# Tokenize the text\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Semantic analysis using Word2Vec\n",
    "semantic_analysis_word2vec = [(word, word2vec_model[word]) for word in words if word in word2vec_model]\n",
    "\n",
    "# Display token text and vector\n",
    "for word, vector in semantic_analysis_word2vec:\n",
    "    print(f\"Word: {word}, Vector: {vector[:5]}\")  # Displaying first 5 dimensions of the vector for brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1beff1-c943-4d7f-b60c-6416eaf836cd",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "\n",
    "What: This snippet demonstrates semantic analysis using NLTK for tokenization and Gensim's Word2Vec model for obtaining word vectors.\n",
    "\n",
    "Why: NLTK is used for tokenization (word_tokenize) to break down the text into words. Gensim's Word2Vec model (KeyedVectors) is then loaded to obtain vector representations for each word.\n",
    "\n",
    "How: After tokenizing the text (words = word_tokenize(text)), the script checks if each word exists in the loaded Word2Vec model (word in word2vec_model). If true, it retrieves the vector representation (word2vec_model[word]) for semantic analysis purposes.\n",
    "\n",
    "## Summary\n",
    "SpaCy offers an efficient way to perform semantic analysis by leveraging pre-trained word vectors.\n",
    "NLTK provides tools for text processing and tokenization, which are essential for preparing text for semantic analysis.\n",
    "Gensim's Word2Vec models enable the extraction of semantic information through word vectors, enhancing the understanding of textual context and meaning.\n",
    "By using these libraries and models, developers can implement robust semantic analysis solutions for various NLP tasks effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a394e204-6565-48e4-ad2e-ead4e459a5ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b19740c-9d67-4a1e-a6f0-794ad2b3d95f",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - '/Users/vidyadharbendre/nltk_data'\n    - '/Users/vidyadharbendre/miniforge3/envs/nlp_env/nltk_data'\n    - '/Users/vidyadharbendre/miniforge3/envs/nlp_env/share/nltk_data'\n    - '/Users/vidyadharbendre/miniforge3/envs/nlp_env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI love this product! It works really well and meets all my expectations.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Initialize Sentiment Intensity Analyzer\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m sid \u001b[38;5;241m=\u001b[39m \u001b[43mSentimentIntensityAnalyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Perform sentiment analysis\u001b[39;00m\n\u001b[1;32m     11\u001b[0m scores \u001b[38;5;241m=\u001b[39m sid\u001b[38;5;241m.\u001b[39mpolarity_scores(text)\n",
      "File \u001b[0;32m~/miniforge3/envs/nlp_env/lib/python3.9/site-packages/nltk/sentiment/vader.py:340\u001b[0m, in \u001b[0;36mSentimentIntensityAnalyzer.__init__\u001b[0;34m(self, lexicon_file)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    338\u001b[0m     lexicon_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    339\u001b[0m ):\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlexicon_file \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlexicon_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlexicon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_lex_dict()\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstants \u001b[38;5;241m=\u001b[39m VaderConstants()\n",
      "File \u001b[0;32m~/miniforge3/envs/nlp_env/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/miniforge3/envs/nlp_env/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m~/miniforge3/envs/nlp_env/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - '/Users/vidyadharbendre/nltk_data'\n    - '/Users/vidyadharbendre/miniforge3/envs/nlp_env/nltk_data'\n    - '/Users/vidyadharbendre/miniforge3/envs/nlp_env/share/nltk_data'\n    - '/Users/vidyadharbendre/miniforge3/envs/nlp_env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Sample text\n",
    "text = \"I love this product! It works really well and meets all my expectations.\"\n",
    "\n",
    "# Initialize Sentiment Intensity Analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Perform sentiment analysis\n",
    "scores = sid.polarity_scores(text)\n",
    "\n",
    "# Interpret the sentiment score\n",
    "if scores['compound'] >= 0.05:\n",
    "    sentiment = \"Positive\"\n",
    "elif scores['compound'] <= -0.05:\n",
    "    sentiment = \"Negative\"\n",
    "else:\n",
    "    sentiment = \"Neutral\"\n",
    "\n",
    "# Display results\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Sentiment: {sentiment}\")\n",
    "print(f\"Scores: {scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae59821-d1e6-4c46-8aea-726d5f62f7a2",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "\n",
    "What: The code snippet demonstrates sentiment analysis using NLTK's VADER (Valence Aware Dictionary and sEntiment Reasoner).\n",
    "Why: NLTK's VADER is chosen for its simplicity and effectiveness in sentiment analysis tasks, especially for social media text and informal language.\n",
    "How: The SentimentIntensityAnalyzer is initialized (sid = SentimentIntensityAnalyzer()), and then sentiment analysis is performed on the sample text (scores = sid.polarity_scores(text)). Based on the compound score (scores['compound']), which represents the overall sentiment, the sentiment label is determined and displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5978755d-36da-4859-9024-aa8d560b052a",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "What are Word Embeddings?\n",
    "Word Embeddings are a type of word representation that allows words with similar meanings to have a similar representation. They capture semantic relationships between words and can be used to find similarities between words or analyze textual context.\n",
    "\n",
    "Why Word Embeddings?\n",
    "Word Embeddings are useful for:\n",
    "\n",
    "Semantic Similarity: Calculating similarity between words based on their vector representations.\n",
    "\n",
    "NLP Tasks: Enhancing performance in various NLP tasks such as machine translation, named entity recognition, and sentiment analysis.\n",
    "Feature Representation: Providing dense, meaningful representations of words for machine learning models.\n",
    "How to Use Word Embeddings Programmatically?\n",
    "                                                                            \n",
    "Using Gensim for Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93a257d2-ccd1-4955-936c-c24b9a09ffdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/vidyadharbendre/nlp_workspace/learn_nlp_using_examples/GoogleNews-vectors-negative300.bin'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_model = '/Users/vidyadharbendre/nlp_workspace/learn_nlp_using_examples/GoogleNews-vectors-negative300.bin'\n",
    "path_to_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e80ebdc4-5504-40a4-aba8-8efd1428673b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: apple, Vector: [-0.06445312 -0.16015625 -0.01208496  0.13476562 -0.22949219]\n",
      "Word: orange, Vector: [-0.10498047 -0.18261719  0.09912109  0.26367188 -0.19628906]\n",
      "Word: fruit, Vector: [-0.05834961  0.06787109 -0.05395508  0.33398438 -0.13574219]\n",
      "Word: computer, Vector: [ 0.10742188 -0.20117188  0.12304688  0.21191406 -0.09130859]\n",
      "Word: phone, Vector: [-0.01446533 -0.12792969 -0.11572266 -0.22167969 -0.07373047]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load pre-trained Word2Vec model (example using a small pre-trained model)\n",
    "# Replace 'path_to_model' with the actual path to your Word2Vec model\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(path_to_model, binary=True)\n",
    "\n",
    "# Example words\n",
    "words = ['apple', 'orange', 'fruit', 'computer', 'phone']\n",
    "\n",
    "# Obtain word embeddings\n",
    "word_embeddings = [(word, word2vec_model[word]) for word in words if word in word2vec_model]\n",
    "\n",
    "# Display word and corresponding vector\n",
    "for word, vector in word_embeddings:\n",
    "    print(f\"Word: {word}, Vector: {vector[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c2048e-c22b-4e88-9fe9-edbb7132dee8",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "\n",
    "What: This snippet demonstrates how to use Gensim to load a pre-trained Word2Vec model and obtain word embeddings.\n",
    "Why: Word embeddings provide meaningful representations of words in a continuous vector space, capturing semantic relationships.\n",
    "How: The Word2Vec model is loaded (word2vec_model = KeyedVectors.load_word2vec_format('path/to/GoogleNews-vectors-negative300.bin', binary=True)), and then word embeddings are obtained for example words (word2vec_model[word]). The resulting vectors can be used for semantic analysis, similarity calculations, or as features in machine learning models.\n",
    "\n",
    "## Summary\n",
    "Sentiment Analysis using NLTK's VADER provides a straightforward way to determine the sentiment expressed in text, useful for various applications like customer feedback analysis.\n",
    "Word Embeddings obtained using Gensim's Word2Vec models enable the representation of words as dense vectors, capturing semantic meanings and relationships, beneficial for enhancing NLP tasks and machine learning applications.\n",
    "By utilizing these libraries and models, developers can effectively perform Sentiment Analysis and leverage Word Embeddings to enhance their NLP solutions and text processing applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4491eaa-9790-4bd2-afaa-f9e96c25aaf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
