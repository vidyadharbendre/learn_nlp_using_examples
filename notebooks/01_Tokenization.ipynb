{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6498fee4-b80a-493f-bff6-5c2fefdb6ec4",
   "metadata": {},
   "source": [
    "<table align=\"left\" width=100%>\n",
    "    <tr>\n",
    "        <td width=\"10%\">\n",
    "            <img src=\"../images/RA_Logo.png\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <div align=\"center\">\n",
    "                <font color=\"#21618C\" size=8px>\n",
    "                  <b> 1. Tokenization </b>\n",
    "                </font>\n",
    "            </div>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ed588-e3b8-4d40-9f1e-caaaf829c514",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/vidyadharbendre/learn_nlp_using_examples/blob/main/notebooks/01_Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/vidyadharbendre/learn_nlp_using_examples/blob/main/notebooks/01_Tokenization.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be13adf6-1ff5-4234-b793-e6bae9921d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f9b3715-29bf-4127-aee4-37b73e2b2288",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a70ee3bc-ff5a-4fa3-a9d8-645f050fa176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "print(numpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c89dc7b6-f8f8-4e7e-919e-0af88cb8f185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.19\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "259ddd8d-59b6-4ce9-a9ff-539b6996cce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: numpy\n",
      "Version: 1.26.4\n",
      "Summary: Fundamental package for array computing in Python\n",
      "Home-page: https://numpy.org\n",
      "Author: Travis E. Oliphant et al.\n",
      "Author-email: \n",
      "License: Copyright (c) 2005-2023, NumPy Developers.\n",
      "        All rights reserved.\n",
      "        \n",
      "        Redistribution and use in source and binary forms, with or without\n",
      "        modification, are permitted provided that the following conditions are\n",
      "        met:\n",
      "        \n",
      "            * Redistributions of source code must retain the above copyright\n",
      "               notice, this list of conditions and the following disclaimer.\n",
      "        \n",
      "            * Redistributions in binary form must reproduce the above\n",
      "               copyright notice, this list of conditions and the following\n",
      "               disclaimer in the documentation and/or other materials provided\n",
      "               with the distribution.\n",
      "        \n",
      "            * Neither the name of the NumPy Developers nor the names of any\n",
      "               contributors may be used to endorse or promote products derived\n",
      "               from this software without specific prior written permission.\n",
      "        \n",
      "        THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n",
      "        \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n",
      "        LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n",
      "        A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n",
      "        OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n",
      "        SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n",
      "        LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n",
      "        DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n",
      "        THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
      "        (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
      "        OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
      "Location: /Users/vidyadharbendre/miniforge3/envs/nlp_env/lib/python3.9/site-packages\n",
      "Requires: \n",
      "Required-by: blis, h5py, spacy, thinc\n"
     ]
    }
   ],
   "source": [
    "!pip show numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d74afaf6-bb6a-49b3-ae35-1af31be47e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.4\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Print the version of SpaCy installed\n",
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "daf18e8b-5e9f-482a-9e6e-5cee6acfe849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n",
      "3.11.0\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import h5py\n",
    "\n",
    "print(numpy.__version__)\n",
    "print(h5py.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "113bf032-6505-4e67-8538-8622fd0d4836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import numpy; print(numpy.__version__)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "415b97d0-c709-45d4-ab53-24a73caa6bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad018d12-92e8-4fdd-a539-ddf25bd6f59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a10eda72-8603-49cd-bf06-afb9a80c8465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84aea42-b27b-4119-948c-b62a9553970c",
   "metadata": {},
   "source": [
    "## Basic Tokenization\n",
    "\n",
    "A simple way to tokenize text is to split it at spaces and punctuation marks. However, this approach can be too simplistic for more complex text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f159a0d3-ee42-49b8-be47-728392d822f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Radhika', 'saved', 'the', 'puppy']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization using Regular Expression\n",
    "\n",
    "text = \"Radhika saved the puppy\"\n",
    "tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d4a7531-7766-4129-b3ee-403c0c1dff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge spacy-model-en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9462d148-04ac-45d3-8fb0-db155c75fce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Radhika', 'saved', 'the', 'puppy']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization using spacy\n",
    "\n",
    "#import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"Radhika saved the puppy\"\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9db637ca-bcfe-48c4-aa6e-44d34451c00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Radhika\n",
      "saved\n",
      "the\n",
      "puppy\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53c300b8-6ba3-41a3-9369-c2c21c9f22fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'puppy'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25018002-b1cf-4fcf-9ac7-73e5ed9721e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Radhika', 'saved', 'the', 'puppy']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ token.text  for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "51ad0812-9e2f-437c-9ed4-80100137b585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/vidyadharbendre/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vidyadharbendre/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization using nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "752272ad-d60b-4106-87cf-358084816cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK version: 3.8.1\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Print the version of NLTK installed\n",
    "print(\"NLTK version:\", nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee4aa7c5-8b90-4085-8912-e33e96361661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Radhika', 'saved', 'the', 'puppy']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Example text\n",
    "text = \"Radhika saved the puppy\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a1041c74-872e-4246-9605-c582967795dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Tokens: ['radhika', 'saved', 'puppy']\n"
     ]
    }
   ],
   "source": [
    "# Normalization\n",
    "# Convert to lower case\n",
    "tokens = [token.lower() for token in tokens]\n",
    "\n",
    "# Remove punctuation\n",
    "tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "print(\"Normalized Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80eed37-c6dc-4727-81f5-5eb35a89d5c7",
   "metadata": {},
   "source": [
    "## Challenges in Tokenization\n",
    "\n",
    "# Non-Alphanumeric Characters\n",
    "Splitting text at all non-alphanumeric characters can help in tokenization, but it may not always be ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "374afb8a-f564-4148-9ab6-527b272a4a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', 'It', 's', 'a', 'beautiful', 'day']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world! It's a beautiful day.\"\n",
    "tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6df32b-6aff-41c0-b6be-4798040a212a",
   "metadata": {},
   "source": [
    "# Apostrophes\n",
    "Handling apostrophes can be tricky as they can signify contractions or possessives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0bed8084-a76f-46ad-a07c-53e0f7738f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It's\", \"Radhika's\", 'puppy']\n"
     ]
    }
   ],
   "source": [
    "text = \"It's Radhika's puppy.\"\n",
    "tokens = re.findall(r\"\\b\\w+(?:'\\w+)?\\b\", text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b31c6a-503e-407f-8a49-d3443bcdd52c",
   "metadata": {},
   "source": [
    "# Two-Word Entities\n",
    "Recognizing two-word entities, such as proper nouns or named entities, requires more advanced techniques like Named Entity Recognition (NER).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ffd2c989-6803-44ce-9a4f-c878bcce7c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Radhika', 'lives', 'in', 'New', 'Delhi', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vidyadharbendre/miniforge3/envs/nlp_env/lib/python3.9/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.5.0) was trained with spaCy v3.5.0 and may not be 100% compatible with the current version (3.7.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "#import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"Radhika lives in New Delhi.\"\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2467ca-aecc-4a27-a983-c2245d01f4b0",
   "metadata": {},
   "source": [
    "# Compound Words\n",
    "Compound words, especially in languages like Sanskrit and German, need careful handling to preserve their meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "399b142c-9520-41e0-8723-e69a2422877c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bhagavad', 'gita']\n"
     ]
    }
   ],
   "source": [
    "text = \"Bhagavad-gita\"\n",
    "tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81df193-32f7-4545-96ab-f1843b2c548f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e86cb9-4473-4363-a0bf-add78dee3a49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0735348f-3b53-4a8a-aba0-870cb7e4601d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
