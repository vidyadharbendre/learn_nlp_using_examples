{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6498fee4-b80a-493f-bff6-5c2fefdb6ec4",
   "metadata": {},
   "source": [
    "<table align=\"left\" width=100%>\n",
    "    <tr>\n",
    "        <td width=\"10%\">\n",
    "            <img src=\"../images/RA_Logo.png\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <div align=\"center\">\n",
    "                <font color=\"#21618C\" size=8px>\n",
    "                  <b> 1. Tokenization </b>\n",
    "                </font>\n",
    "            </div>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ed588-e3b8-4d40-9f1e-caaaf829c514",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/vidyadharbendre/learn_nlp_using_examples/blob/main/notebooks/01_Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/vidyadharbendre/learn_nlp_using_examples/blob/main/notebooks/01_Tokenization.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be13adf6-1ff5-4234-b793-e6bae9921d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f9b3715-29bf-4127-aee4-37b73e2b2288",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad018d12-92e8-4fdd-a539-ddf25bd6f59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84aea42-b27b-4119-948c-b62a9553970c",
   "metadata": {},
   "source": [
    "## Basic Tokenization\n",
    "\n",
    "A simple way to tokenize text is to split it at spaces and punctuation marks. However, this approach can be too simplistic for more complex text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f159a0d3-ee42-49b8-be47-728392d822f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Radhika', 'saved', 'the', 'puppy']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization using Regular Expression\n",
    "\n",
    "text = \"Radhika saved the puppy\"\n",
    "tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9462d148-04ac-45d3-8fb0-db155c75fce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Radhika', 'saved', 'the', 'puppy']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization using spacy\n",
    "\n",
    "#import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"Radhika saved the puppy\"\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9db637ca-bcfe-48c4-aa6e-44d34451c00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Radhika\n",
      "saved\n",
      "the\n",
      "puppy\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53c300b8-6ba3-41a3-9369-c2c21c9f22fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'puppy'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25018002-b1cf-4fcf-9ac7-73e5ed9721e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Radhika', 'saved', 'the', 'puppy']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ token.text  for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "51ad0812-9e2f-437c-9ed4-80100137b585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/vidyadharbendre/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vidyadharbendre/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization using nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee4aa7c5-8b90-4085-8912-e33e96361661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Radhika', 'saved', 'the', 'puppy']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Example text\n",
    "text = \"Radhika saved the puppy\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a1041c74-872e-4246-9605-c582967795dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Tokens: ['radhika', 'saved', 'puppy']\n"
     ]
    }
   ],
   "source": [
    "# Normalization\n",
    "# Convert to lower case\n",
    "tokens = [token.lower() for token in tokens]\n",
    "\n",
    "# Remove punctuation\n",
    "tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "print(\"Normalized Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80eed37-c6dc-4727-81f5-5eb35a89d5c7",
   "metadata": {},
   "source": [
    "## Challenges in Tokenization\n",
    "\n",
    "# Non-Alphanumeric Characters\n",
    "Splitting text at all non-alphanumeric characters can help in tokenization, but it may not always be ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "374afb8a-f564-4148-9ab6-527b272a4a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', 'It', 's', 'a', 'beautiful', 'day']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world! It's a beautiful day.\"\n",
    "tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6df32b-6aff-41c0-b6be-4798040a212a",
   "metadata": {},
   "source": [
    "# Apostrophes\n",
    "Handling apostrophes can be tricky as they can signify contractions or possessives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0bed8084-a76f-46ad-a07c-53e0f7738f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It's\", \"Radhika's\", 'puppy']\n"
     ]
    }
   ],
   "source": [
    "text = \"It's Radhika's puppy.\"\n",
    "tokens = re.findall(r\"\\b\\w+(?:'\\w+)?\\b\", text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b31c6a-503e-407f-8a49-d3443bcdd52c",
   "metadata": {},
   "source": [
    "# Two-Word Entities\n",
    "Recognizing two-word entities, such as proper nouns or named entities, requires more advanced techniques like Named Entity Recognition (NER).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffd2c989-6803-44ce-9a4f-c878bcce7c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Radhika', 'lives', 'in', 'New', 'Delhi', '.']\n"
     ]
    }
   ],
   "source": [
    "#import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"Radhika lives in New Delhi.\"\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2467ca-aecc-4a27-a983-c2245d01f4b0",
   "metadata": {},
   "source": [
    "# Compound Words\n",
    "Compound words, especially in languages like Sanskrit and German, need careful handling to preserve their meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "399b142c-9520-41e0-8723-e69a2422877c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bhagavad', 'gita']\n"
     ]
    }
   ],
   "source": [
    "text = \"Bhagavad-gita\"\n",
    "tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81df193-32f7-4545-96ab-f1843b2c548f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
